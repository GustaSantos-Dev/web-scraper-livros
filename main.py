import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ydata_profiling import ProfileReport  # AtualizaÃ§Ã£o da importaÃ§Ã£o
from datetime import datetime
import schedule
import time

# ğŸ“¥ Coleta o conteÃºdo HTML de uma URL
def pegar_html(url):
    try:
        resposta = requests.get(url)
        if resposta.status_code == 200:
            return resposta.text
        else:
            print(f"Erro ao acessar o site: {resposta.status_code}")
            return None
    except Exception as e:
        print(f"Ocorreu um erro: {e}")
        return None

# ğŸ” Extrai livros do HTML
def coletar_livros(html):
    soup = BeautifulSoup(html, "html.parser")
    livros = []
    itens = soup.find_all("article", class_="product_pod")

    for item in itens:
        titulo = item.h3.a["title"]
        preco = item.find("p", class_="price_color").text
        livros.append({"TÃ­tulo": titulo, "PreÃ§o": preco})

    return livros

# â¡ï¸ Verifica se hÃ¡ prÃ³xima pÃ¡gina
def encontrar_proxima_pagina(soup):
    next_button = soup.find("li", class_="next")
    if next_button:
        return next_button.a["href"]
    return None

# ğŸ” Faz scraping de vÃ¡rias pÃ¡ginas
def scraping_varias_paginas():
    base_url = "https://books.toscrape.com/catalogue/category/books/data-science_22/"
    url = base_url + "index.html"
    livros_coletados = []

    while url:
        print(f"ğŸ“„ Coletando: {url}")
        html = pegar_html(url)
        if html:
            soup = BeautifulSoup(html, "html.parser")
            livros = coletar_livros(html)
            livros_coletados.extend(livros)
            proxima = encontrar_proxima_pagina(soup)
            url = base_url + proxima if proxima else None
        else:
            break

    return livros_coletados

# ğŸ’¾ Salva em CSV
def salvar_em_csv(lista):
    agora = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    nome = f"livros_{agora}.csv"
    df = pd.DataFrame(lista)
    df.to_csv(nome, index=False)
    print(f"ğŸ“ CSV salvo como: {nome}")
    return nome

# ğŸ“Š Gera grÃ¡fico com Seaborn
def gerar_grafico(lista):
    df = pd.DataFrame(lista)
    df["PreÃ§o"] = df["PreÃ§o"].str.replace("Â£", "").astype(float)

    plt.figure(figsize=(10, 5))
    sns.histplot(df["PreÃ§o"], bins=10, kde=True, color="skyblue")
    plt.title("DistribuiÃ§Ã£o de PreÃ§os dos Livros")
    plt.xlabel("PreÃ§o (Â£)")
    plt.ylabel("FrequÃªncia")
    plt.grid(True)
    plt.tight_layout()
    plt.savefig("grafico_precos.png")
    print("ğŸ“ˆ GrÃ¡fico salvo como: grafico_precos.png")

# ğŸ“‹ Gera dashboard automÃ¡tico
def gerar_dashboard(nome_arquivo_csv):
    df = pd.read_csv(nome_arquivo_csv)
    df["PreÃ§o"] = df["PreÃ§o"].str.replace("Â£", "").astype(float)

    profile = ProfileReport(df, title="AnÃ¡lise de Livros", explorative=True)
    profile.to_file("dashboard_livros.html")
    print("ğŸ“Š Dashboard salvo como: dashboard_livros.html")

# ğŸš€ FunÃ§Ã£o principal
def main():
    print("ğŸ” Iniciando scraping...")
    livros = scraping_varias_paginas()
    if livros:
        arquivo_csv = salvar_em_csv(livros)
        gerar_grafico(livros)
        gerar_dashboard(arquivo_csv)
        print("âœ… Processo completo com sucesso!")
    else:
        print("âš ï¸ Nenhum dado encontrado.")

# â±ï¸ Agendamento diÃ¡rio
schedule.every().day.at("09:00").do(main)

# ğŸ” Loop contÃ­nuo
print("â³ Sistema de scraping agendado. Pressione Ctrl+C para parar.")
while True:
    schedule.run_pending()
    time.sleep(60)

